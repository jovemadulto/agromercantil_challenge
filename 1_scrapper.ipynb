{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coleta de dados Externos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import OleFileIO_PL\n",
    "from datetime import datetime\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.webdriver import WebDriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.common.exceptions import NoSuchWindowException\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navegação\n",
    "\n",
    "- O Selenium foi escolhido como ferramenta de navegação devido à incapacidade do portal `CEPEA.org.br` de ser renderizado corretamente por bibliotecas mais simples, como Requests e BeautifulSoup. Isso ocorre porque o portal utiliza tecnologias que exigem um navegador completo para exibir o conteúdo.\n",
    "\n",
    "- Apesar dessa limitação técnica, o portal foi selecionado por ser uma **fonte primária** de dados e por apresentar uma **metodologia transparente** para consulta, o que aumenta a confiabilidade e a qualidade das informações obtidas.\n",
    "\n",
    "- O portal apresenta os seguintes desafios reais para web scraping:\n",
    "    - Necessidade de renderização JavaScript para exibir o conteúdo corretamente.\n",
    "    - Dados distribuídos de forma dispersa nas páginas.\n",
    "    - Ausência de uma API para acesso estruturado aos dados.\n",
    "    - Disponibilidade limitada de dados diretamente na página, restrita aos últimos 15 dias.\n",
    "    - Entrega dinâmica dos dados, realizada após uma requisição ao banco de dados, em formato fechado (`xls`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "download_folder = Path('raw_data') / datetime.now().strftime('%Y-%m-%d')\n",
    "if not download_folder.exists():\n",
    "    download_folder.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_options = Options()\n",
    "# chrome_options.add_argument(\"--headless\")  # Executar em modo headless (opcional)\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "# Configurar o diretório de download automático\n",
    "\n",
    "prefs = {\n",
    "    \"download.default_directory\": str(download_folder.absolute()),\n",
    "    \"download.prompt_for_download\": False,\n",
    "    \"download.directory_upgrade\": True,\n",
    "    \"safebrowsing.enabled\": True\n",
    "}\n",
    "\n",
    "chrome_options.add_experimental_option(\"prefs\", prefs)\n",
    "chrome_service = ChromeService()\n",
    "\n",
    "driver = webdriver.Chrome(service=chrome_service, options=chrome_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_str(string: str) -> str:\n",
    "    \"\"\"Normaliza uma string, removendo acentos e convertendo para minúsculas.\n",
    "    A função aplica a normalização Unicode NFKD para separar os caracteres de \n",
    "    suas marcas diacríticas (acentos), remove essas marcas e, em seguida, \n",
    "    converte a string resultante para a sua forma 'casefolded' (uma versão \n",
    "    mais agressiva de minúsculas, ideal para comparações).\n",
    "    Args:\n",
    "        string (str): A string de entrada a ser normalizada.\n",
    "    Returns:\n",
    "        str: A string normalizada, sem acentos e em minúsculas.\n",
    "    \"\"\"\n",
    "\n",
    "    import unicodedata\n",
    "\n",
    "    try:\n",
    "        normalized = unicodedata.normalize(\"NFKD\", string)\n",
    "        string = \"\".join([c for c in normalized if not unicodedata.combining(c)])\n",
    "        string = string.casefold()\n",
    "        return string\n",
    "    except TypeError:\n",
    "        return None\n",
    "\n",
    "def fetch_categories(webdriver: webdriver, url: str) -> dict:\n",
    "    \"\"\"\n",
    "    Busca categorias e seus URLs correspondentes de uma página da web.\n",
    "    Esta função utiliza um Selenium WebDriver para navegar até a URL especificada,\n",
    "    extrai uma lista de categorias da página e retorna um dicionário que mapeia\n",
    "    os nomes das categorias para seus URLs. Categorias não relacionadas à agricultura\n",
    "    são excluídas dos resultados.\n",
    "    Argumentos:\n",
    "        webdriver (selenium.webdriver): A instância do Selenium WebDriver usada para interagir com a página da web.\n",
    "        url (str): A URL da página da web de onde as categorias serão extraídas.\n",
    "    Retorna:\n",
    "        dict: Um dicionário onde as chaves são os nomes normalizados das categorias (str)\n",
    "              e os valores são seus URLs correspondentes (str).\n",
    "    Notas:\n",
    "        - A função assume que as categorias estão localizadas em um XPath específico\n",
    "          (`/html/body/div/div[2]/div/div[2]`) e que cada categoria é representada como\n",
    "          um elemento `<li>` contendo uma tag `<a>` com o URL.\n",
    "        - Categorias não relacionadas à agricultura, como 'bezerro', 'boi', 'florestal',\n",
    "          'frango', 'ovinos', 'ovos', 'suino' e 'tilapia', são excluídas dos resultados.\n",
    "    \"\"\"\n",
    "    categories_dict = {}\n",
    "\n",
    "    driver.get(url)\n",
    "    categories = driver.find_element(By.XPATH, '/html/body/div/div[2]/div/div[2]')\n",
    "    for category in categories.find_elements(By.TAG_NAME, 'li'):\n",
    "        category_name = category.text\n",
    "        category_name = normalize_str(category_name)\n",
    "        category_url = category.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
    "        # Exclui categorias indesejadas, não relacionadas a agricultura\n",
    "        #\n",
    "        # Exclui também categorias relacionadas a mandioca e feijão\n",
    "        # Estas, ainda que relacionadas à agricultura, após\n",
    "        # avanços no projeto, se mostraram excessivamente custosas de manejar\n",
    "        # neste momento, visto que necessitam de tratamento especial e entendimento\n",
    "        # extra do contexto\n",
    "        if category_name not in ['bezerro', 'boi', 'florestal', 'frango', 'leite', 'ovinos', 'ovos', 'suino', 'tilapia', 'feijao', 'mandioca']:\n",
    "            categories_dict[category_name] = category_url\n",
    "    return categories_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_download_URLs(webdriver: webdriver, category_URL: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Busca URLs de download em uma página de categoria.\n",
    "\n",
    "    Esta função utiliza o Selenium WebDriver para acessar a página da URL fornecida,\n",
    "    localiza todos os links (<a> tags) na página e retorna uma lista contendo os URLs\n",
    "    que possuem a palavra 'series' em seu atributo 'href'.\n",
    "\n",
    "    Args:\n",
    "        category_URL (str): A URL da página da categoria onde os links de download serão buscados.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: Uma lista de strings contendo os URLs de download encontrados na página.\n",
    "\n",
    "    Notas:\n",
    "        - Certifique-se de que o WebDriver esteja configurado corretamente para acessar a página.\n",
    "    \"\"\"\n",
    "    download_URLs = []\n",
    "    driver.get(category_URL)\n",
    "    for a_tag in driver.find_elements(By.TAG_NAME, 'a'):\n",
    "        href = a_tag.get_attribute('href')\n",
    "        if href and 'series' in href:\n",
    "            download_URLs.append(href)\n",
    "    return download_URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = fetch_categories(driver, \"https://www.cepea.org.br/br\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dos dados brutos\n",
    "\n",
    "Como afirmado anteriormente, os dados brutos estão disponíveis para download no site do CEPEA somente em formatos fechados, como XLS.\n",
    "\n",
    "Portanto, é necessário baixar esses arquivos e convertê-los para um formato aberto, como CSV ou Parquet, para facilitar o processamento e a análise dos dados.\n",
    "\n",
    "Em primeiro lugar, o download dos dados segue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acessando página de preços de [Acucar] na URL: https://www.cepea.org.br/br/indicador/acucar.aspx\n",
      "Acessando página de preços de [Algodao] na URL: https://www.cepea.org.br/br/indicador/algodao.aspx\n",
      "Acessando página de preços de [Arroz] na URL: https://www.cepea.org.br/br/indicador/arroz.aspx\n",
      "Acessando página de preços de [Cafe] na URL: https://www.cepea.org.br/br/indicador/cafe.aspx\n",
      "Acessando página de preços de [Citros] na URL: https://www.cepea.org.br/br/indicador/citros.aspx\n",
      "Acessando página de preços de [Etanol] na URL: https://www.cepea.org.br/br/indicador/etanol.aspx\n",
      "Acessando página de preços de [Hortifruti] na URL: https://www.cepea.org.br/br/hortifruti.aspx\n",
      "Acessando página de preços de [Milho] na URL: https://www.cepea.org.br/br/indicador/milho.aspx\n",
      "Acessando página de preços de [Soja] na URL: https://www.cepea.org.br/br/indicador/soja.aspx\n",
      "Acessando página de preços de [Trigo] na URL: https://www.cepea.org.br/br/indicador/trigo.aspx\n"
     ]
    }
   ],
   "source": [
    "_to_download = []\n",
    "\n",
    "for cat_name, cat_url in categories.items():\n",
    "    print(f\"Acessando página de preços de [{cat_name.title()}] na URL: {cat_url}\") \n",
    "    download_links = fetch_download_URLs(driver, cat_url)\n",
    "    _to_download.extend(download_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_files(driver, _to_download):\n",
    "    def download_files(driver: WebDriver, _to_download: List[str]) -> None:\n",
    "        \"\"\"\n",
    "        Inicia o download de arquivos abrindo uma nova aba para cada link fornecido.\n",
    "\n",
    "        Args:\n",
    "            driver (selenium.webdriver.chrome.webdriver.WebDriver): O WebDriver do Selenium usado para interagir com o navegador.\n",
    "            _to_download (List[str]): Uma lista de URLs de download.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "    driver.switch_to.window(driver.window_handles[0])\n",
    "    # Cria uma aba para cada link de download:\n",
    "    for link in tqdm(_to_download[:], desc=\"Iniciando downloads\", unit=\"arquivo\"):\n",
    "        driver.execute_script(f\"window.open('{link}');\")\n",
    "        # Pequena pausa para evitar sobrecarga do servidor\n",
    "        # Testes mostraram que 7 segundos é um tempo adequado\n",
    "        # E que 8 segundos é o tempo médio para o download completar\n",
    "        sleep(3)\n",
    "    print('Todos os downloads foram iniciados.')\n",
    "\n",
    "\n",
    "def check_tabs(driver):\n",
    "    \"\"\"\n",
    "    Verifica e gerencia as abas abertas em um navegador controlado pelo Selenium WebDriver.\n",
    "    Este método realiza as seguintes ações:\n",
    "    - Aguarda um intervalo de tempo antes de iniciar a verificação das abas.\n",
    "    - Itera por todas as abas abertas no navegador.\n",
    "    - Identifica abas com URLs contendo 'id' e as adiciona a uma lista de abas de download.\n",
    "    - Verifica se o título da aba contém '524', indicando falha no download, e tenta reabrir a URL correspondente.\n",
    "    - Fecha as abas que foram processadas com sucesso.\n",
    "    - Repete o processo até que todas as abas sejam fechadas com sucesso.\n",
    "    Args:\n",
    "        driver (selenium.webdriver.remote.webdriver.WebDriver): \n",
    "            Instância do WebDriver do Selenium usada para controlar o navegador.\n",
    "    Raises:\n",
    "        selenium.common.exceptions.NoSuchWindowException: \n",
    "            Caso uma aba não seja encontrada durante a troca de contexto.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        print('Iniciando o processo de checagem de abas.')\n",
    "        all_windows = driver.window_handles\n",
    "        print(f\"Número de abas abertas: {len(all_windows)}\")\n",
    "        \n",
    "        download_windows = []\n",
    "        for window in all_windows:\n",
    "            try:\n",
    "                driver.switch_to.window(window)\n",
    "            except NoSuchWindowException:\n",
    "                print(f\"Aba {window} não encontrada. Download finalizado.\")\n",
    "                continue\n",
    "            if 'id' in driver.current_url:\n",
    "                download_windows.append(window)\n",
    "            page_title = driver.title\n",
    "            \n",
    "            # Abas que contém o título com '524' indicam falha no download em razão da CloudFlare\n",
    "            # e precisam ser reabertas\n",
    "            if '524' in page_title:\n",
    "                failed_download_URL = driver.current_url\n",
    "                print(f\"Erro encontrado na aba {window}, tentando novamente\")\n",
    "                driver.execute_script(\"window.close();\")\n",
    "                driver.execute_script(f\"window.open('{failed_download_URL}');\")\n",
    "        if not download_windows:\n",
    "            print(\"Todas as abas foram processadas e fechadas.\")\n",
    "            break\n",
    "        sleep(10)  # Espera antes de checar as abas novamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iniciando downloads:   0%|          | 0/14 [00:00<?, ?arquivo/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iniciando downloads: 100%|██████████| 14/14 [00:42<00:00,  3.02s/arquivo]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Todos os downloads foram iniciados.\n",
      "Iniciando o processo de checagem de abas.\n",
      "Número de abas abertas: 10\n",
      "Aba 43D1E155BBFE241E7C61879D6BDBEC94 não encontrada. Download finalizado.\n",
      "Aba CAC728DBFF1FB86E8E0021AAAE948D11 não encontrada. Download finalizado.\n",
      "Aba 4EF65DE650182FCE4A6318442C336D8F não encontrada. Download finalizado.\n",
      "Aba 331A7564BB77AA62E5BAAB12452987E0 não encontrada. Download finalizado.\n",
      "Aba 646832EBBF8F100995872EDE80977DEF não encontrada. Download finalizado.\n",
      "Aba D6FB46486513331822F42F5228E86ED3 não encontrada. Download finalizado.\n",
      "Aba 35D60930D9F53BE695096AAD2538CD43 não encontrada. Download finalizado.\n",
      "Aba 638DC6D18E9802F61AFA4243B73DB6B2 não encontrada. Download finalizado.\n",
      "Todas as abas foram processadas e fechadas.\n"
     ]
    }
   ],
   "source": [
    "download_files(driver, _to_download)\n",
    "check_tabs(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estruturação da Camada RAW\n",
    "\n",
    "- Os dados obtidos do CEPEA não estão em um formato padrão `xls`.\n",
    "    - Eles são armazenados como um [`Compound Document`](https://en.wikipedia.org/wiki/Compound_File_Binary_Format), um formato de arquivo container que permite armazenar diferentes tipos de conteúdo em um único arquivo.\n",
    "- O Pandas, por padrão, não suporta este tipo de arquivo. Por isso, é necessário utilizar a biblioteca `OleFileIO_PL` para acessar o conteúdo interno e fazer o parsing utilizando as engines `xlrd` - para arquivos no formato `.xlrd` - ou `openpyxl` - para arquivos no formato `.xlsx`.\n",
    "    - Essa biblioteca permite abrir e manipular arquivos no formato OLE (Object Linking and Embedding), que é a base do formato Compound Document.\n",
    "\n",
    "[Referência: https://stackoverflow.com/a/63347276]\n",
    "\n",
    "- Os arquivos convertidos são salvos em uma pasta chamada `raw_data`, que é criada automaticamente caso não exista no momento da execução.\n",
    "- O formato `.csv` foi escolhido para armazenar os dados convertidos, em vez de `.json` ou `.parquet`, pelos seguintes motivos:\n",
    "    - Alguns arquivos apresentam inconsistências no formato, como colunas excedentes ou diferentes estruturas, o que dificulta a padronização.\n",
    "        - Por exemplo, arquivos como `precos_farinha_mandioca_seca_grossa_branca_crua_tipo_1.csv` possuem mais colunas do que a maioria dos outros arquivos.\n",
    "    - O formato `.csv` é simples, amplamente suportado e não apresenta a repetição de chaves a cada linha, como ocorre no `.json`.\n",
    "    - Embora o formato `.parquet` seja mais eficiente em termos de armazenamento e leitura em grandes volumes de dados, ele depende de bibliotecas especializadas devido à sua natureza binária, o que pode dificultar o processo de depuração.\n",
    "    - O tamanho dos dados neste projeto não justifica a adoção do `.parquet`, que é mais vantajoso em cenários de Big Data.\n",
    "\n",
    "- Esta etapa de estruturação da camada RAW prioriza simplicidade e flexibilidade, considerando o tamanho atual dos dados e a necessidade de facilitar o processamento e a análise subsequente. O formato `.csv` atende bem a esses requisitos, oferecendo um equilíbrio entre acessibilidade e funcionalidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = Path('clean_data')\n",
    "if not clean_data.exists():\n",
    "    clean_data.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_xls_to_csv(download_folder: Path, clean_data_folder: Path) -> None:\n",
    "    \"\"\"\n",
    "    Converte arquivos .xls em .csv no diretório especificado.\n",
    "\n",
    "    Esta função percorre todos os arquivos .xls no diretório fornecido,\n",
    "    converte cada um deles para o formato .csv e salva o arquivo convertido\n",
    "    no em um diretório separado com o nome indicado no cabeçalho.\n",
    "\n",
    "    Args:\n",
    "        raw_data_path (Path): O caminho do diretório onde os arquivos .xls estão localizados.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    headers_to_fname = {\n",
    "        \"INDICADOR DO AÇÚCAR CRISTAL BRANCO CEPEA/ESALQ - SÃO PAULO\": \"acucar_cristal_branco_cepea_esalq_sao_paulo\",\n",
    "        \"Indicador do Algodão em Pluma CEPEA/ESALQ - Prazo de 8 dias\": \"algodao_pluma_cepea_esalq_8_dias\",\n",
    "        \"INDICADOR DO CAFÉ ARÁBICA CEPEA/ESALQ\": \"indicador_cafe_arabica_cepea_esalq\",\n",
    "        \"INDICADOR DO ARROZ EM CASCA CEPEA/IRGA-RS\": \"indicador_arroz_casca_cepea_irga_rs\",\n",
    "        \"INDICADOR DO CAFÉ ROBUSTA CEPEA/ESALQ\": \"indicador_cafe_robusta_cepea_esalq\",\n",
    "        \"Indicador Açúcar Cristal - Santos (FOB)\": \"indicador_acucar_cristal_santos_fob\",\n",
    "        \"Preços do Feijão Carioca - Notas 8 e 8,5 - CEPEA/CNA\": \"precos_feijao_carioca_notas_8_85_cepea_cna\",\n",
    "        \"Preços do Feijão Carioca - peneira 12 e/ou notas 9 ou superior - CEPEA/CNA\": \"precos_feijao_carioca_peneira_12_notas_9_cepea_cna\",\n",
    "        \"Indicador Semanal do Etanol Hidratado Outros Fins CEPEA/ESALQ - São Paulo\": \"indicador_semanal_etanol_hidratado_outros_fins_cepea_esalq_sao_paulo\",\n",
    "        \"PREÇOS DA FARINHA DE MANDIOCA SECA GROSSA - BRANCA/CRUA TIPO 1\": \"precos_farinha_mandioca_seca_grossa_branca_crua_tipo_1\",\n",
    "        \"INDICADOR DO MILHO ESALQ/BM&FBOVESPA\": \"indicador_milho_esalq_bm_fbovespa\",\n",
    "        \"INDICADOR DA SOJA CEPEA/ESALQ - PARANAGUÁ\": \"indicador_soja_cepea_esalq_paranagua\",\n",
    "        \"PREÇO MÉDIO DO TRIGO CEPEA/ESALQ - PARANÁ\": \"preco_medio_trigo_cepea_esalq_parana\",\n",
    "        \"PREÇO MÉDIO DO TRIGO CEPEA/ESALQ - RIO GRANDE DO SUL\": \"preco_medio_trigo_cepea_esalq_rio_grande_sul\",\n",
    "        \"LEITE AO PRODUTOR CEPEA/ESALQ (R$/litro) - líquido\": \"leite_ao_produtor_cepea_esalq_r_litro_liquido\",\n",
    "        \"PREÇOS DA RAIZ DE MANDIOCA\": \"precos_raiz_mandioca\",\n",
    "        \"INDICADOR DA SOJA CEPEA/ESALQ - PARANÁ\": \"indicador_soja_cepea_esalq_parana\",\n",
    "        \"PREÇOS DA FARINHA DE MANDIOCA SECA FINA - BRANCA/CRUA TIPO 1\": \"precos_farinha_mandioca_seca_fina_branca_crua_tipo_1\",\n",
    "        \"INDICADOR SEMANAL DO ETANOL HIDRATADO COMBUSTÍVEL CEPEA/ESALQ - SÃO PAULO\": \"indicador_semanal_etanol_hidratado_combustivel_cepea_esalq_sao_paulo\",\n",
    "        \"PREÇOS DA FÉCULA DE MANDIOCA\": \"precos_fecula_mandioca\",\n",
    "        \"Preços do Feijão Preto Tipo 1 - CEPEA/CNA\": \"precos_feijao_preto_tipo_1_cepea_cna\",\n",
    "        \"INDICADOR SEMANAL DO ETANOL ANIDRO CEPEA/ESALQ - SÃO PAULO\": \"indicador_semanal_etanol_anidro_cepea_esalq_sao_paulo\",\n",
    "\n",
    "    }\n",
    "\n",
    "    xls_files = list(download_folder.glob('*.xls'))\n",
    "    for xls_file in tqdm(xls_files, desc=\"Convertendo arquivos .xls para .csv\", unit=\"arquivo\"):\n",
    "        ole_file = OleFileIO_PL.OleFileIO(xls_file)\n",
    "        df = pd.read_excel(ole_file.openstream('Workbook'), engine='xlrd')\n",
    "        \n",
    "        df_header = df.columns[0]\n",
    "        # Utiliza o cabeçalho do arquivo como um filename mais descritivo\n",
    "        # Se o cabeçalho não estiver no dicionário, usa o nome do arquivo original\n",
    "        # Caso o nome do arquivo não conste no dicionário, registra um aviso no logger\n",
    "        if df_header not in headers_to_fname:\n",
    "            print(f\"Aviso: Cabeçalho '{df_header}' não encontrado no dicionário. Usando o nome original do arquivo.\")\n",
    "        filename = headers_to_fname.get(df_header, f\"{xls_file.stem}.csv\") + '.csv'\n",
    "        \n",
    "\n",
    "        real_df = df[3:].copy() # Pula as 3 primeiras linhas que são cabeçalhos desnecessários\n",
    "        real_df.columns = df.iloc[2].values # Define a 3ª linha como cabeçalho do novo arquivo\n",
    "        real_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        real_df.columns = [normalize_str(col) for col in real_df.columns] # Normaliza os nomes das colunas\n",
    "        real_df.columns = [col.replace(' ', '_') for col in real_df.columns] # Substitui espaços por underscores nos nomes das colunas\n",
    "        real_df.columns = [col.replace('r$', 'brl') for col in real_df.columns] # Substitui 'r$' por 'brl' nos nomes das colunas\n",
    "        real_df.columns = [col.replace('us$', 'usd') for col in real_df.columns] # Substitui 'us$' por 'usd' nos nomes das colunas\n",
    "        real_df.columns = [col.replace('.', '') for col in real_df.columns] # Remove pontos dos nomes das colunas\n",
    "        \n",
    "        real_df.to_csv(clean_data / filename, index=False, encoding='cp1252')\n",
    "        ole_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Convertendo arquivos .xls para .csv: 100%|██████████| 14/14 [00:00<00:00, 18.46arquivo/s]\n"
     ]
    }
   ],
   "source": [
    "convert_xls_to_csv(download_folder=download_folder, clean_data_folder=clean_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Armazenamento na AWS S3\n",
    "\n",
    "A Amazon possui algumas diretivas de recomendação no caso de adoção da solução AWS S3, disponível em [sua documentação](https://docs.aws.amazon.com/prescriptive-guidance/latest/defining-bucket-names-data-lakes/data-layer-definitions.html).\n",
    "\n",
    "No caso da camada Raw deste projeto:\n",
    "\n",
    "- Para armazenar os dados brutos em um Bucket S3 na AWS, o recomendável seria criar um bucket dedicado, ou caso já exista um, uma pasta exclusiva para esta tarefa dentro dele.\n",
    "- Dentro deste bucket deverão ser armazenados os dados sem transformação alguma, ou seja, no caso específico deste projeto, os arquivos localizados em `/raw_data`, provenientes da função `download_files()`.\n",
    "    - Apesar de seu formato impróprio para consumo direto em razão dos pontos já levantados, eles representam a versão mais próxima da realidade no momento da exportação do banco original.\n",
    "- Os arquivos, de preferência, devem ser organizados em subpastas para melhor organização.\n",
    "    - No caso deste projeto, como espera-se que eles sejam exportados uma vez por dia, faz sentido que sejam criadas subpastas dentro de `/raw_data` com o dia de execução do script.\n",
    "- É necessário ativar a função de versionamento do bucket também, para evitar perdas em caso de sobrescrita.\n",
    "- É necessário também identificar os usuários e seus papéis na organização e configurar o Bucket para que somente os indivíduos que necessitam de autorização para acessar e modificar os arquivos consigam fazê-lo.\n",
    "- A implementação de uma política de ciclo de vida também é recomendável, uma vez que a documentação indica que arquivos mais antigos podem ser movidos para soluções de armazenamento de longa duração e pouco acesso, como a Glacier ou IA (Infrequent Access)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agromercantil",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
